{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 426,
      "metadata": {
        "id": "J-SOCfgJtHJw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#plt.style.use('seaborn-notebook')\n",
        "sns.set_theme(context='notebook')\n",
        "sns.set_style(\"whitegrid\")\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "id": "s0cEpP6BtPSO"
      },
      "outputs": [],
      "source": [
        "class Action(IntEnum):\n",
        "    up = 0\n",
        "    right = 1\n",
        "    down = 2\n",
        "    left = 3\n",
        "\n",
        "action_to_str = {\n",
        "    Action.up : \"up\",\n",
        "    Action.right : \"right\",\n",
        "    Action.down : \"down\",\n",
        "    Action.left : \"left\",\n",
        "}\n",
        "\n",
        "action_to_offset = {\n",
        "    Action.up : (-1, 0),\n",
        "    Action.right : (0, 1),\n",
        "    Action.down : (1, 0),\n",
        "    Action.left : (0, -1),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 428,
      "metadata": {
        "id": "nvTUR66UtPUk"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "\n",
        "    def __init__(self, height, width, goal, goal_value=5.0, danger=[], danger_value=-5.0, blocked=[], noise=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the GridWorld environment.\n",
        "        Creates a gridworld like MDP\n",
        "         - height (int): Number of rows\n",
        "         - width (int): Number of columns\n",
        "         - goal (int): Index number of goal cell\n",
        "         - goal_value (float): Reward given for goal cell\n",
        "         - danger (list of int): Indices of cells marked as danger\n",
        "         - danger_value (float): Reward given for danger cell\n",
        "         - blocked (list of int): Indices of cells marked as blocked (can't enter)\n",
        "         - noise (float): probability of resulting state not being what was expected\n",
        "        \"\"\"\n",
        "\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grid_values = [0 for _ in range(height * width)] # Initialize state values.\n",
        "        self._goal_value = goal_value\n",
        "        self._danger_value = danger_value\n",
        "        self._goal_cell = goal\n",
        "        self._danger_cells = danger\n",
        "        self._blocked_cells = blocked\n",
        "        self._noise = noise # Noise level in the environment.\n",
        "        assert noise >= 0 and noise < 1 # Ensure valid noise value.\n",
        "        self.create_next_values() # Initialize the next state values.\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state values to their initial state.\n",
        "        \"\"\"\n",
        "        self._grid_values = [0 for _ in range(self._height * self._width)]\n",
        "        self.create_next_values()\n",
        "\n",
        "\n",
        "    def _inbounds(self, state):\n",
        "        \"\"\"\n",
        "        Check if a state index is within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state >= 0 and state < self._width * self._height\n",
        "\n",
        "    def _inbounds_rc(self, state_r, state_c):\n",
        "        \"\"\"\n",
        "        Check if row and column indices are within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state_r >= 0 and state_r < self._height and state_c >= 0 and state_c < self._width\n",
        "\n",
        "    def _state_to_rc(self, state):\n",
        "        \"\"\"\n",
        "        Convert a state index to row and column indices.\n",
        "        \"\"\"\n",
        "        return state // self._width, state % self._width\n",
        "\n",
        "    def _state_from_action(self, state, action):\n",
        "        \"\"\"\n",
        "        Gets the state as a result of applying the given action\n",
        "        \"\"\"\n",
        "        row, col = self._state_to_rc(state)\n",
        "        dir_row, dir_col = action_to_offset[action] # returns the directional offset\n",
        "\n",
        "        new_row = row + dir_row\n",
        "        new_col = col + dir_col\n",
        "\n",
        "        if not self._inbounds_rc(new_row, new_col):\n",
        "            return state  # bump into outer edge > stay in place\n",
        "        \n",
        "        if (new_row, new_col) in {self._state_to_rc(b) for b in self._blocked_cells}:\n",
        "            return state # return current state if walking into blocked\n",
        "\n",
        "        return new_row * self._width + new_col # convert back into original state form\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\"\n",
        "        Returns true if a state is terminal (goal, or danger)\n",
        "        \"\"\"\n",
        "        return state in self._danger_cells or state == self._goal_cell\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"\n",
        "        Gets all non-terminal states in the environment\n",
        "        \"\"\"\n",
        "        return [x for x in range(self._width*self._height) if (x not in self._blocked_cells) and (not self.is_terminal(x))]\n",
        "\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        \"\"\"\n",
        "        Returns a list of valid actions given the current state\n",
        "        \"\"\"\n",
        "        # if the state is blocked or terminal, then we cant take any actions\n",
        "        if state in self._blocked_cells:\n",
        "            return []\n",
        "        if self.is_terminal(state):\n",
        "            return []\n",
        "        return [Action.up, Action.right, Action.down, Action.left]\n",
        "\n",
        "\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for being in the current state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        # Reward is non-zero for danger or goal\n",
        "        #TO DO:\n",
        "        if state == self._goal_cell:\n",
        "            return self._goal_value\n",
        "        elif state in self._danger_cells:\n",
        "            return self._danger_value\n",
        "        \n",
        "        return 0\n",
        "\n",
        "\n",
        "    def get_transitions(self, state, action):\n",
        "        \"\"\"\n",
        "        Get a list of transitions as a result of attempting the action in the current state\n",
        "        Each item in the list is a dictionary, containing the probability of reaching that state and the state itself\n",
        "        \"\"\"\n",
        "        #Assume action input is Action.___\n",
        "        transitions = dict() # start with an empty dict\n",
        "        possible_actions = [Action.up, Action.right, Action.down, Action.left]\n",
        "\n",
        "        # Add the main state/action pair\n",
        "        main_state = self._state_from_action(state, action)\n",
        "        transitions[main_state] = 1 - self._noise\n",
        "\n",
        "        #distribution noise\n",
        "        noise_prob = self._noise / 3\n",
        "        for a in possible_actions: # iterate threough all actions\n",
        "\n",
        "            if a == action: #if we inserted this action alr.\n",
        "                continue\n",
        "\n",
        "            next_state = self._state_from_action(state, a)\n",
        "\n",
        "            if next_state in transitions:\n",
        "                transitions[next_state] += noise_prob\n",
        "            else:\n",
        "                transitions[next_state] = noise_prob\n",
        "            \n",
        "\n",
        "        return transitions\n",
        "\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Get the current value of the state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        return self._grid_values[state]\n",
        "\n",
        "    def create_next_values(self):\n",
        "        \"\"\"\n",
        "        Creates a temporary storage for state value updating\n",
        "        If this is not used, then asynchronous updating may result in unexpected results\n",
        "        To use properly, run this at the start of each iteration\n",
        "        \"\"\"\n",
        "        self._new_grid_values = deepcopy(self._grid_values) #creates a temp storage value assigned to the GridWorld class\n",
        "\n",
        "    def set_next_values(self):\n",
        "        \"\"\"\n",
        "        Set the state values from the temporary copied values\n",
        "        To use properly, run this at the end of each iteration\n",
        "        \"\"\"\n",
        "        # imagine like a full commit of these new values\n",
        "        self._grid_values = self._new_grid_values\n",
        "        \n",
        "\n",
        "    def set_value(self, state, value):\n",
        "        \"\"\"\n",
        "        Set the value of the state into the temporary copy\n",
        "        This value will not update into main storage until self.set_next_values() is called.\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state) #Ensure we are in bounds and pass in the assigned value\n",
        "        self._new_grid_values[state] = value\n",
        "\n",
        "    def solve_linear_system(self, discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Solve the gridworld using a system of linear equations.\n",
        "        :param discount_factor: The discount factor for future rewards.\n",
        "        \"\"\"\n",
        "\n",
        "        states = list(range(self._width*self._height))\n",
        "\n",
        "        #define an empty A and b to be solved\n",
        "        A = np.zeros((len(states), len(states)))\n",
        "        b = np.zeros(len(states))\n",
        "\n",
        "        index = {s: i for i, s in enumerate(states)} # map states to equation indicies\n",
        "\n",
        "        for st in states: # we iterate for every state to form this matrix\n",
        "            i = index[st]\n",
        "            \n",
        "            #blocked states (value is 0)\n",
        "            if st in self._blocked_cells:\n",
        "                A[i,i] = 1.0\n",
        "                b[i] = 0.0\n",
        "                continue\n",
        "\n",
        "            # terminal states\n",
        "            if self.is_terminal(st):\n",
        "                A[i,i] = 1.0\n",
        "                b[i] = 0.0\n",
        "                continue\n",
        "\n",
        "            # iterate through all possible actions in a NORMAL state\n",
        "            A[i,i] = 1\n",
        "            pi = 1.0 / len(self.get_actions(st))\n",
        "\n",
        "            for a in self.get_actions(st):\n",
        "                t = self.get_transitions(st,a) # gets the transitions of an action\n",
        "\n",
        "                #iterate through each transition\n",
        "                for next_st, p in t.items():\n",
        "                    j = index[next_st]\n",
        "                    A[i,j] -= discount_factor * pi * p \n",
        "                    b[i] += pi * p * self.get_reward(next_st) # expected immediate reward on entering next state\n",
        "\n",
        "        # solve the matrix\n",
        "        V = np.linalg.solve(A,b)\n",
        "\n",
        "        for s, i in index.items():\n",
        "            self._grid_values[s] = V[i]\n",
        "\n",
        "        return V\n",
        "    \n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Pretty print the state values\n",
        "        \"\"\"\n",
        "        out_str = \"\"\n",
        "        for r in range(self._height):\n",
        "            for c in range(self._width):\n",
        "                cell = r * self._width + c\n",
        "                if cell in self._blocked_cells:\n",
        "                    out_str += \"{:>6}\".format(\"----\")\n",
        "                elif cell == self._goal_cell:\n",
        "                    out_str += \"{:>6}\".format(\"GOAL\")\n",
        "                elif cell in self._danger_cells:\n",
        "                    out_str += \"{:>6.2f}\".format(self._danger_value)\n",
        "                else:\n",
        "                    out_str += \"{:>6.2f}\".format(self._grid_values[cell])\n",
        "                out_str += \" \"\n",
        "            out_str += \"\\n\"\n",
        "        return out_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {
        "id": "Dfgo0v5sNO78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -2.69  -3.60  -5.00  -1.64  -0.00 \n",
            " -2.35   ----   ----  -0.00   1.64 \n",
            " -2.51   ----   ----  -0.00   GOAL \n",
            " -3.19  -3.91  -4.10  -5.00  -0.57 \n",
            " -3.82  -5.00  -3.99  -3.45  -1.82 \n",
            "\n",
            "------\n",
            "[-2.69193992e+00 -3.59873473e+00  0.00000000e+00 -1.63934426e+00\n",
            " -2.73077267e-16 -2.35186931e+00  0.00000000e+00  0.00000000e+00\n",
            " -1.64139136e-16  1.63934426e+00 -2.50692908e+00  0.00000000e+00\n",
            "  0.00000000e+00 -4.19813847e-17  0.00000000e+00 -3.18976339e+00\n",
            " -3.90994964e+00 -4.10007492e+00  0.00000000e+00 -5.65479377e-01\n",
            " -3.82394058e+00  0.00000000e+00 -3.99029090e+00 -3.44770113e+00\n",
            " -1.81548642e+00]\n"
          ]
        }
      ],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "\n",
        "# Solve the linear system\n",
        "values_grid = simple_gw.solve_linear_system(discount_factor=0.95)\n",
        "print(simple_gw)\n",
        "print(\"------\")\n",
        "print(values_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 430,
      "metadata": {
        "id": "HJI-fizhOIM-"
      },
      "outputs": [],
      "source": [
        "def value_iteration(gw, discount, tolerance=0.1):\n",
        "    \n",
        "    states = list(range(gw._width * gw._height))\n",
        "\n",
        "    for it in range(1000):\n",
        "        gw.create_next_values()\n",
        "        d = 0.0 # delta\n",
        "\n",
        "        for s in states:\n",
        "            old_v = gw.get_value(s)\n",
        "\n",
        "            if s in gw._blocked_cells: # blocked\n",
        "                new_v = 0.0\n",
        "\n",
        "            elif gw.is_terminal(s): #terminal\n",
        "                new_v = 0.0 # terminal states have zero future value\n",
        "\n",
        "            else: # other states\n",
        "                best_q = -np.inf # king of the hill style checking\n",
        "\n",
        "                for a in gw.get_actions(s):\n",
        "                    q = 0.0\n",
        "                    for s2, p in gw.get_transitions(s,a).items():\n",
        "                        r = gw.get_reward(s2) # gain reward on entry\n",
        "                        q += p * (r + discount * gw.get_value(s2)) # reward of current state plus next state over all actions\n",
        "                    best_q = max(best_q, q) # we want the best value always\n",
        "                \n",
        "                new_v = best_q\n",
        "\n",
        "            gw.set_value(s, new_v) # assign our winning value to the state\n",
        "            d = max(d, abs(new_v - old_v))\n",
        "\n",
        "        gw.set_next_values()\n",
        "\n",
        "        # stopping condition\n",
        "        if d < tolerance:\n",
        "            break\n",
        "    \n",
        "    return np.array([gw.get_value(s) for s in states])\n",
        "\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "id": "OwueBZR8tPXE"
      },
      "outputs": [],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "noisy_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.2)\n",
        "discount = 0.95\n",
        "tolerance = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {
        "id": "Yli-IAo6tPZU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  3.15   2.99  -5.00   4.51   4.75 \n",
            "  3.32   ----   ----   4.75   5.00 \n",
            "  3.49   ----   ----   5.00   GOAL \n",
            "  3.68   3.87   4.07  -5.00   5.00 \n",
            "  3.49  -5.00   4.29   4.51   4.75 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "value_iteration(simple_gw, discount, tolerance)\n",
        "print(simple_gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 433,
      "metadata": {
        "id": "uUo6u5kutPbm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0.56   0.11  -5.00   3.60   4.51 \n",
            "  0.64   ----   ----   4.49   4.88 \n",
            "  0.69   ----   ----   4.22   GOAL \n",
            "  0.74   0.83   1.40  -5.00   4.17 \n",
            "  0.26  -5.00   2.10   2.90   3.84 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "value_iteration(noisy_gw, discount, tolerance)\n",
        "print(noisy_gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -0.78  -2.24  -5.00  -1.54  -0.00 \n",
            " -0.38   ----   ----  -0.00   1.54 \n",
            " -0.48   ----   ----  -0.00   GOAL \n",
            " -1.22  -2.45  -2.73  -5.00  -0.17 \n",
            " -2.37  -5.00  -2.71  -2.34  -0.75 \n",
            "\n",
            "^vLinear Solve:\n",
            "[-7.84458160e-01 -2.23533745e+00  0.00000000e+00 -1.53846154e+00\n",
            " -1.00814287e-16 -3.79523086e-01  0.00000000e+00  0.00000000e+00\n",
            " -7.24676930e-17  1.53846154e+00 -4.80618794e-01  0.00000000e+00\n",
            "  0.00000000e+00 -3.63708493e-17  0.00000000e+00 -1.22253956e+00\n",
            " -2.45029076e+00 -2.72872040e+00  0.00000000e+00 -1.73834091e-01\n",
            " -2.36676187e+00  0.00000000e+00 -2.70749766e+00 -2.33710278e+00\n",
            " -7.53281062e-01]\n",
            "\n",
            " Value Iteration:\n",
            "Simple:\n",
            "  0.38   0.28  -5.00   2.81   3.75 \n",
            "  0.50   ----   ----   3.75   5.00 \n",
            "  0.67   ----   ----   5.00   GOAL \n",
            "  0.89   1.19   1.58  -5.00   5.00 \n",
            "  0.67  -5.00   2.11   2.81   3.75 \n",
            "\n",
            "Noisy:\n",
            " -0.02  -0.39  -5.00   1.82   3.13 \n",
            " -0.00   ----   ----   3.18   4.54 \n",
            " -0.01   ----   ----   4.03   GOAL \n",
            " -0.04  -0.37  -0.03  -5.00   4.00 \n",
            " -0.40  -5.00   0.54   1.41   2.75 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 0.75 discount!\n",
        "discount = 0.75\n",
        "# Initialize your GridWorld\n",
        "simple_gw2 = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "\n",
        "# Solve the linear system\n",
        "values_grid = simple_gw2.solve_linear_system(discount_factor=0.75)\n",
        "print(simple_gw2)\n",
        "print(\"^vLinear Solve:\")\n",
        "print(values_grid)\n",
        "\n",
        "# ---------------\n",
        "print(\"\\n Value Iteration:\")\n",
        "simple_gw3 = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "noisy_gw2 = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.2)\n",
        "\n",
        "print(\"Simple:\")\n",
        "value_iteration(simple_gw3, discount, tolerance)\n",
        "print(simple_gw3)\n",
        "\n",
        "print(\"Noisy:\")\n",
        "value_iteration(noisy_gw2, discount, tolerance)\n",
        "print(noisy_gw2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
