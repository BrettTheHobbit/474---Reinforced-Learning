{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "J-SOCfgJtHJw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#plt.style.use('seaborn-notebook')\n",
        "sns.set_theme(context='notebook')\n",
        "sns.set_style(\"whitegrid\")\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "s0cEpP6BtPSO"
      },
      "outputs": [],
      "source": [
        "class Action(IntEnum):\n",
        "    up = 0\n",
        "    right = 1\n",
        "    down = 2\n",
        "    left = 3\n",
        "\n",
        "action_to_str = {\n",
        "    Action.up : \"up\",\n",
        "    Action.right : \"right\",\n",
        "    Action.down : \"down\",\n",
        "    Action.left : \"left\",\n",
        "}\n",
        "\n",
        "action_to_offset = {\n",
        "    Action.up : (-1, 0),\n",
        "    Action.right : (0, 1),\n",
        "    Action.down : (1, 0),\n",
        "    Action.left : (0, -1),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "nvTUR66UtPUk"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "\n",
        "    def __init__(self, height, width, goal, goal_value=5.0, danger=[], danger_value=-5.0, blocked=[], noise=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the GridWorld environment.\n",
        "        Creates a gridworld like MDP\n",
        "         - height (int): Number of rows\n",
        "         - width (int): Number of columns\n",
        "         - goal (int): Index number of goal cell\n",
        "         - goal_value (float): Reward given for goal cell\n",
        "         - danger (list of int): Indices of cells marked as danger\n",
        "         - danger_value (float): Reward given for danger cell\n",
        "         - blocked (list of int): Indices of cells marked as blocked (can't enter)\n",
        "         - noise (float): probability of resulting state not being what was expected\n",
        "        \"\"\"\n",
        "\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grid_values = [0 for _ in range(height * width)] # Initialize state values.\n",
        "        self._goal_value = goal_value\n",
        "        self._danger_value = danger_value\n",
        "        self._goal_cell = goal\n",
        "        self._danger_cells = danger\n",
        "        self._blocked_cells = blocked\n",
        "        self._noise = noise # Noise level in the environment.\n",
        "        assert noise >= 0 and noise < 1 # Ensure valid noise value.\n",
        "        self.create_next_values() # Initialize the next state values.\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state values to their initial state.\n",
        "        \"\"\"\n",
        "        self._grid_values = [0 for _ in range(self._height * self._width)]\n",
        "        self.create_next_values()\n",
        "\n",
        "\n",
        "    def _inbounds(self, state):\n",
        "        \"\"\"\n",
        "        Check if a state index is within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state >= 0 and state < self._width * self._height\n",
        "\n",
        "    def _inbounds_rc(self, state_r, state_c):\n",
        "        \"\"\"\n",
        "        Check if row and column indices are within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state_r >= 0 and state_r < self._height and state_c >= 0 and state_c < self._width\n",
        "\n",
        "    def _state_to_rc(self, state):\n",
        "        \"\"\"\n",
        "        Convert a state index to row and column indices.\n",
        "        \"\"\"\n",
        "        return state // self._width, state % self._width\n",
        "\n",
        "    def _state_from_action(self, state, action):\n",
        "        \"\"\"\n",
        "        Gets the state as a result of applying the given action\n",
        "        \"\"\"\n",
        "        row, col = self._state_to_rc(state)\n",
        "        dir_row, dir_col = action_to_offset[action] # returns the directional offset\n",
        "\n",
        "        new_row = row + dir_row\n",
        "        new_col = col + dir_col\n",
        "\n",
        "        if not self._inbounds_rc(new_row, new_col):\n",
        "            return state  # bump into outer edge > stay in place\n",
        "        \n",
        "        if (new_row, new_col) in {self._state_to_rc(b) for b in self._blocked_cells}:\n",
        "            return state # return current state if walking into blocked\n",
        "\n",
        "        return new_row * self._width + new_col # convert back into original state form\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\"\n",
        "        Returns true if a state is terminal (goal, or danger)\n",
        "        \"\"\"\n",
        "        return state in self._danger_cells or state == self._goal_cell\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"\n",
        "        Gets all non-terminal states in the environment\n",
        "        \"\"\"\n",
        "        return [x for x in range(self._width*self._height) if x not in self._danger_cells and x != self._goal_cell]\n",
        "\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        \"\"\"\n",
        "        Returns a list of valid actions given the current state\n",
        "        \"\"\"\n",
        "        #Start with a full list, remove actions that lead to illegal states\n",
        "        actions = [Action.up, Action.right, Action.down, Action.left]\n",
        "\n",
        "        s_row, s_col = self._state_to_rc(state) # cconvert state to row,col format\n",
        "        blocked = [self._state_to_rc(x) for x in self._blocked_cells]\n",
        "        \n",
        "\n",
        "        # check bounds and illegal states\n",
        "        if (s_row - 1 < 0 or ((s_row-1, s_col) in blocked)): # Up\n",
        "            actions.remove(Action.up)\n",
        "\n",
        "        if (s_col + 1 >= self._width or ((s_row, s_col+1) in blocked)): # Right\n",
        "            actions.remove(Action.right)\n",
        "\n",
        "        if (s_row + 1 >= self._height or ((s_row+1, s_col) in blocked)): # Down\n",
        "            actions.remove(Action.down)\n",
        "\n",
        "        if (s_col - 1 < 0 or ((s_row, s_col-1) in blocked)): # Left\n",
        "            actions.remove(Action.left)\n",
        "\n",
        "        return actions\n",
        "\n",
        "\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for being in the current state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        # Reward is non-zero for danger or goal\n",
        "        #TO DO:\n",
        "        if state == self._goal_cell:\n",
        "            return self._goal_value\n",
        "        elif state in self._danger_cells:\n",
        "            return self._danger_value\n",
        "        \n",
        "        return 0\n",
        "\n",
        "\n",
        "    def get_transitions(self, state, action):\n",
        "        \"\"\"\n",
        "        Get a list of transitions as a result of attempting the action in the current state\n",
        "        Each item in the list is a dictionary, containing the probability of reaching that state and the state itself\n",
        "        \"\"\"\n",
        "        #Assume action input is Action.___\n",
        "        transitions = dict() # start with an empty dict\n",
        "        possible_actions = [Action.up, Action.right, Action.down, Action.left]\n",
        "\n",
        "        # Add the main state/action pair\n",
        "        main_state = self._state_from_action(state, action)\n",
        "        transitions[main_state] = 1 - self._noise\n",
        "\n",
        "        #distribution noise\n",
        "        noise_prob = self._noise / 3\n",
        "        for a in possible_actions: # iterate threough all actions\n",
        "\n",
        "            if a == action: #if we inserted this action alr.\n",
        "                continue\n",
        "\n",
        "            next_state = self._state_from_action(state, a)\n",
        "\n",
        "            if next_state in transitions:\n",
        "                transitions[next_state] += noise_prob\n",
        "            else:\n",
        "                transitions[next_state] = noise_prob\n",
        "            \n",
        "\n",
        "        return transitions\n",
        "\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Get the current value of the state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        return self._grid_values[state]\n",
        "\n",
        "    def create_next_values(self):\n",
        "        \"\"\"\n",
        "        Creates a temporary storage for state value updating\n",
        "        If this is not used, then asynchronous updating may result in unexpected results\n",
        "        To use properly, run this at the start of each iteration\n",
        "        \"\"\"\n",
        "        self._new_grid_values = deepcopy(self._grid_values) #creates a temp storage value assigned to the GridWorld class\n",
        "\n",
        "    def set_next_values(self):\n",
        "        \"\"\"\n",
        "        Set the state values from the temporary copied values\n",
        "        To use properly, run this at the end of each iteration\n",
        "        \"\"\"\n",
        "        # imagine like a full commit of these new values\n",
        "        self._grid_values = self._new_grid_values\n",
        "        \n",
        "\n",
        "    def set_value(self, state, value):\n",
        "        \"\"\"\n",
        "        Set the value of the state into the temporary copy\n",
        "        This value will not update into main storage until self.set_next_values() is called.\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state) #Ensure we are in bounds and pass in the assigned value\n",
        "        self._new_grid_values[state] = value\n",
        "\n",
        "    def solve_linear_system(self, discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Solve the gridworld using a system of linear equations.\n",
        "        :param discount_factor: The discount factor for future rewards.\n",
        "        \"\"\"\n",
        "\n",
        "        states = list(range(self._width*self._height))\n",
        "\n",
        "        #define an empty A and b to be solved\n",
        "        A = np.zeros((len(states), len(states)))\n",
        "        b = np.zeros(len(states))\n",
        "\n",
        "        index = {s: i for i, s in enumerate(states)} # map states to equation indicies\n",
        "\n",
        "        for st in states: # we iterate for every state to form this matrix\n",
        "            i = index[st]\n",
        "            \n",
        "            #blocked states (value is 0)\n",
        "            if st in self._blocked_cells:\n",
        "                A[i,i] = 1\n",
        "                b[i] = 0\n",
        "                continue\n",
        "\n",
        "            # terminal states\n",
        "            if self.is_terminal(st):\n",
        "                A[i,i] = 1\n",
        "                b[i] = self.get_reward(st)\n",
        "                continue\n",
        "\n",
        "            # iterate through all possible actions in a NORMAL state\n",
        "            A[i,i] = 1\n",
        "            b[i] = self.get_reward(st)\n",
        "            pi = 1.0 / len(self.get_actions(st))\n",
        "\n",
        "            for a in self.get_actions(st):\n",
        "                t = self.get_transitions(st,a) # gets the transitions of an action\n",
        "\n",
        "                #iterate through each transition\n",
        "                for next_st, p in t.items():\n",
        "                    j = index[next_st]\n",
        "                    A[i,j] -= discount_factor * pi * p\n",
        "\n",
        "        # solve the matrix\n",
        "        V = np.linalg.solve(A,b)\n",
        "\n",
        "        for s, i in index.items():\n",
        "            self._grid_values[s] = V[i]\n",
        "\n",
        "        return V\n",
        "    \n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Pretty print the state values\n",
        "        \"\"\"\n",
        "        out_str = \"\"\n",
        "        for r in range(self._height):\n",
        "            for c in range(self._width):\n",
        "                cell = r * self._width + c\n",
        "                if cell in self._blocked_cells:\n",
        "                    out_str += \"{:>6}\".format(\"----\")\n",
        "                elif cell == self._goal_cell:\n",
        "                    out_str += \"{:>6}\".format(\"GOAL\")\n",
        "                elif cell in self._danger_cells:\n",
        "                    out_str += \"{:>6.2f}\".format(self._danger_value)\n",
        "                else:\n",
        "                    out_str += \"{:>6.2f}\".format(self._grid_values[cell])\n",
        "                out_str += \" \"\n",
        "            out_str += \"\\n\"\n",
        "        return out_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "Dfgo0v5sNO78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -3.36  -3.97  -5.00  -1.58   0.00 \n",
            " -3.10   ----   ----   0.00   1.58 \n",
            " -3.16   ----   ----   0.00   GOAL \n",
            " -3.56  -4.01  -4.11  -5.00  -0.61 \n",
            " -4.07  -5.00  -3.98  -3.46  -1.93 \n",
            "\n",
            "------\n",
            "[-3.35549069e+00 -3.96885808e+00 -5.00000000e+00 -1.58333333e+00\n",
            "  0.00000000e+00 -3.09533285e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  1.58333333e+00 -3.16099951e+00  0.00000000e+00\n",
            "  0.00000000e+00  2.58375373e-16  5.00000000e+00 -3.55940297e+00\n",
            " -4.01350399e+00 -4.11482015e+00 -5.00000000e+00 -6.11823407e-01\n",
            " -4.06571641e+00 -5.00000000e+00 -3.98066491e+00 -3.45570063e+00\n",
            " -1.93207392e+00]\n"
          ]
        }
      ],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "\n",
        "# Solve the linear system\n",
        "values_grid = simple_gw.solve_linear_system(discount_factor=0.95)\n",
        "print(simple_gw)\n",
        "print(\"------\")\n",
        "print(values_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "HJI-fizhOIM-"
      },
      "outputs": [],
      "source": [
        "def value_iteration(gw, discount, tolerance=0.1):\n",
        "    \n",
        "    states = list(range(gw._width * gw._height))\n",
        "\n",
        "    for it in range(1000):\n",
        "        gw.create_next_values()\n",
        "        d = 0.0 # delta\n",
        "\n",
        "        for s in states:\n",
        "\n",
        "            if s in gw._blocked_cells: # blocked\n",
        "                new_v = 0.0\n",
        "\n",
        "            elif gw.is_terminal(s): #terminal\n",
        "                new_v = gw.get_reward(s)\n",
        "\n",
        "            else: # other states\n",
        "                r = gw.get_reward(s)\n",
        "                best_q = -np.inf # koth style checking\n",
        "\n",
        "                for a in gw.get_actions(s):\n",
        "                    exp_v = 0.0\n",
        "                    for s2, p in gw.get_transitions(s,a).items():\n",
        "                        exp_v += p*gw.get_value(s2) # calculate the expected value given the probability and the next state for all actions and state transitions\n",
        "                    q = r + discount * exp_v\n",
        "                    if q > best_q:\n",
        "                        best_q = q # the highest expected q is assigned to a state\n",
        "\n",
        "                \n",
        "                new_v = best_q\n",
        "            gw.set_value(s, new_v) # assign our winning value to the state\n",
        "\n",
        "            d = max(d, abs(new_v - gw.get_value(s)))\n",
        "\n",
        "        gw.set_next_values()\n",
        "\n",
        "        # stopping condition\n",
        "\n",
        "        if d < tolerance:\n",
        "            break\n",
        "    \n",
        "    return np.array([gw.get_value(s) for s in states])\n",
        "\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "OwueBZR8tPXE"
      },
      "outputs": [],
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "noisy_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.2)\n",
        "discount = 0.95\n",
        "tolerance = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "Yli-IAo6tPZU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  2.99   2.84  -5.00   4.29   4.51 \n",
            "  3.15   ----   ----   4.51   4.75 \n",
            "  3.32   ----   ----   4.75   GOAL \n",
            "  3.49   3.68   3.87  -5.00   4.75 \n",
            "  3.32  -5.00   4.07   4.29   4.51 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "value_iteration(simple_gw, discount, 0.1)\n",
        "print(simple_gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "uUo6u5kutPbm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0.33  -0.18  -5.00   3.42   4.28 \n",
            "  0.48   ----   ----   4.26   4.63 \n",
            "  0.59   ----   ----   4.01   GOAL \n",
            "  0.67   0.78   1.33  -5.00   3.97 \n",
            "  0.20  -5.00   1.99   2.76   3.65 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "value_iteration(noisy_gw, discount, 0.1)\n",
        "print(noisy_gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uXK_mV9tPdp"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(gw, discount, tolerance=0.1):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "    Output:\n",
        "\n",
        "    evaluate, then iterate upon the policy\n",
        "    \"\"\"\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
